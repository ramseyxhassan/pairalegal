Let me walk you through how the components of our system work together to process insurance queries and deliver accurate, state-specific results.

We begin by collecting data using the SERFF Web Scraper, which pulls regulatory filings from state-specific endpoints. To ensure efficiency, we utilize parallel scraping techniques, allowing us to simultaneously gather data from different types of insurance filings—such as homeowners and auto insurance—across multiple states. This significantly reduces the time it takes to update our dataset. By automating form submissions and navigation using Selenium, the scraper ensures that we collect the most up-to-date regulatory information. This is crucial for businesses that need to stay compliant with changing insurance regulations in real-time.

After data collection, we organize the documents by key categories such as insurance type, business name, and regulatory state. This initial organization is important because it allows us to streamline the subsequent processing and quickly retrieve filings based on specific business or regulatory needs. For example, if an insurance professional needs information specifically for “State Farm” or for a “Homeowners” policy type, this pre-categorization allows us to easily identify and retrieve relevant documents.

Next, we move to the embedding process, where we convert the insurance text into a numerical format that can be efficiently stored and searched. Before generating these embeddings, we apply document chunking to split large insurance filings into smaller sections that are easier for the model to process. We also use embedding overlap, where each chunk shares some overlapping content with the previous one, ensuring that critical context is preserved across sections.

Once chunked, the documents are passed through a SentenceTransformer model to generate embeddings. These embeddings capture the semantic meaning of each document section, allowing for precise and efficient searches. We also implement an importance scoring system, which assigns higher weights to more critical sections—such as those containing policy numbers, coverage details, or specific state regulations—ensuring that the most relevant parts of the document are prioritized during searches.

The generated embeddings are stored in Qdrant, a high-performance vector database. Qdrant organizes the data as vectors and uses cosine distance for similarity searches, enabling us to retrieve highly relevant documents quickly. Additionally, Qdrant's HNSW (Hierarchical Navigable Small World) algorithm further optimizes search speed, making it well-suited for handling large volumes of insurance filings.

When a user submits a query, the system embeds the query and matches it against the document embeddings stored in Qdrant. By applying vector overlap, we ensure that search results include relevant sections that retain important context from neighboring sections. We also use score thresholds to filter out low-confidence results, providing users with the most accurate and relevant information.

Finally, the embedded query and relevant context are passed to the LLaMA model, which has been fine-tuned specifically for insurance queries. The model generates a detailed response based on the retrieved context, ensuring that the answers are grounded in insurance regulations. This response is then delivered through our intuitive front-end interface, making it easy for insurance professionals to quickly find the information they need without manually sifting through lengthy documents.

Overall, our system combines advanced techniques like parallel web scraping, document chunking, embedding overlap, and vector-based similarity search to deliver fast, accurate, and contextually relevant information. This not only saves time but also improves decision-making, helping businesses stay compliant with ever-changing insurance regulations.